---
title: 'Methods for consistent generation of long video'
date: 2025-09-21
permalink: /posts/2025/09/blog2
excerpt: ""
---

<!-- Table -->
对于长视频生成来说，目前对条件帧（历史帧）的利用主要分为：

1. Sliding Window的方法：StreamingT2V
2. 选择的方法：
   1. 人工筛选关键镜头：LCT
   2. 启发式规则，例如相机FOV：WORLDMEM、Context as Memory
   3. 利用上下文和生成帧特征来反应重要性（相似度）：Mixture of Context
3. 整体利用提取特征：
   1. 启发式规则，对远近上下文采用大小Patch：FAR、FramePack
   2. 利用测试时训练使用网络来记忆上下文特征：VideoTTT

此外，通过总结一些全序列固定长度视频生成的方法，也得到一些启发，类似于FullDiT2单独使用一个模块来判断kv重要性来进行压缩、FreeLong使用滤波器来综合高低频信息、StroyDIffusion的Batch帧特征融合（随机❌）；

## Paper Review

- WorldMem：对条件帧利用FOV进行筛选并作为Key-Value，生成帧作为Query提取条件
    Motivation: 有限的上下文时序窗口会限制生成长视频的时序一致性，尤其是保持3D空间结构方面,简单的添加上下文窗口会导致显存和计算不高效
- LoViC: 使用多个复制的Learnable tokens通过自注意力机制来提取上下文特征，通过与去噪帧拼接计算自注意力机制来融合
    Motivation:
    1. DiT注意力的平方复杂度
    2. 使用类似Qformer的结构来提取上下文信息面临两个困难
        交叉注意力层难以兼容RoPE
        固定长度的可学习query会导致对更长上下文捕捉不好
- LCT: 人工筛选条件帧，拼接条件帧和生成帧，利用全注意力机制生成多镜头视频
    Motivation:使用单场景生成模型生成多场景视频,使用外观条件的方法无法保持光照色彩等一致性、使用关键帧生成的方法无法保证跨场景的一致性